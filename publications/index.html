<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><meta charset=utf-8><title>Publications |
Xin Yu
</title><meta name=generator content="Hugo 0.128.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Xin Yu"><meta name=description content="University of Hong Kong"><link rel=stylesheet href=/hugo-anatole/scss/main.min.34631cd94790e1e4245efb06a74614c1cef6b497dfc867de38083a8ad8d523c6.css integrity="sha256-NGMc2UeQ4eQkXvsGp0YUwc72tJffyGfeOAg6itjVI8Y=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/hugo-anatole/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/hugo-anatole/fontawesome/css/fontawesome.min.137b1cf3cea9a8adb7884343a9a5ddddf4280f59153f74dc782fb7f7bf0d0519.css integrity="sha256-E3sc886pqK23iENDqaXd3fQoD1kVP3TceC+3978NBRk=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/hugo-anatole/fontawesome/css/solid.min.e65dc5b48fb5f39b142360c57c3a215744c94e56c755c929cc3e88fe12aab4d3.css integrity="sha256-5l3FtI+185sUI2DFfDohV0TJTlbHVckpzD6I/hKqtNM=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/hugo-anatole/fontawesome/css/regular.min.6f4f16d58da1c82c0c3a3436e021a3d39b4742f741192c546e73e947eacfd92f.css integrity="sha256-b08W1Y2hyCwMOjQ24CGj05tHQvdBGSxUbnPpR+rP2S8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/hugo-anatole/fontawesome/css/brands.min.e10425ad768bc98ff1fb272a0ac8420f9d1ba22f0612c08ff1010c95080ffe7e.css integrity="sha256-4QQlrXaLyY/x+ycqCshCD50boi8GEsCP8QEMlQgP/n4=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/hugo-anatole/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/hugo-anatole/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/hugo-anatole/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/hugo-anatole/favicons/favicon-16x16.png><link rel=canonical href=https://goofish-shop.github.io/hugo-anatole/publications/><link rel=alternate type=application/rss+xml href=/hugo-anatole/publications/index.xml title="My blog"><script type=text/javascript src=/hugo-anatole/js/customer.min.70eb6bc76636a854df8c087cfbe7526ceceda41c35bc4f6c8aa296a96d9eeb65.js integrity="sha256-cOtrx2Y2qFTfjAh8++dSbOztpBw1vE9siqKWqW2e62U=" crossorigin=anonymous></script><script type=text/javascript src=/hugo-anatole/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://goofish-shop.github.io/hugo-anatole/images/site-feature-image.png"><meta name=twitter:title content="Publications"><meta name=twitter:description content="University of Hong Kong"><meta property="og:url" content="https://goofish-shop.github.io/hugo-anatole/publications/"><meta property="og:site_name" content="My blog"><meta property="og:title" content="Publications"><meta property="og:description" content="University of Hong Kong"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:image" content="https://goofish-shop.github.io/hugo-anatole/images/site-feature-image.png"></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img src=/hugo-anatole/images/profile.jpg alt="profile picture" style=width:127px;height:127px;border-radius:30%><div class=sidebar__introduction-title><a href=/hugo-anatole>Xin Yu</a></div><div class=sidebar__introduction-description><p>University of Hong Kong</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://de.linkedin.com/ target=_blank rel="noopener me" aria-label=Linkedin title=Linkedin><i class="fa-brands fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/lxndrblz/anatole/ target=_blank rel="noopener me" aria-label=GitHub title=GitHub><i class="fa-brands fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://www.instagram.com/ target=_blank rel="noopener me" aria-label=instagram title=instagram><i class="fa-brands fa-instagram fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=mailto:mail@example.com target=_blank rel="noopener me" aria-label=e-mail title=e-mail><i class="fa-solid fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Xin Yu
2024</li></ul></footer><script type=text/javascript src=/hugo-anatole/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/hugo-anatole/ title>About</a></li><li class=nav__list-item><a href=/hugo-anatole/post/ title>Blog</a></li><li class=nav__list-item><a class=nav__link--active href=/hugo-anatole/publications/ title>Publication</a></li><li class=nav__list-item><a href=/hugo-anatole/cv/ title>CV</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><h2 class=portfolio__title id=publications>Publications</h2><h4 onclick=onHide() id=descript-title-new>Click here for compressed view</h4><div class=portfolio><div class="portfolio__image-wrapper
portfolio__image-wrapper--right"><a href=https://cvmi-lab.github.io/TEXGen/ target=_blank rel=noopener><img class=portfolio__image src=/hugo-anatole/images/publications/1.jpg alt="EXGen: a Generative Diffusion Model for Mesh Textures"></a></div><div class="portfolio__description
portfolio__description--left"><h2>EXGen: a Generative Diffusion Model for Mesh Textures</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><li class=portfolio__meta-item><em class="fas fa-arrow-down"></em>
<span>2024</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Ze Yuan,</span></li><span class=portfolio__meta-item-authors>Yuanchen Guo,</span></li><span class=portfolio__meta-item-authors>Yingtian Liu,</span></li><span class=portfolio__meta-item-authors>Jianhui Liu,</span></li><span class=portfolio__meta-item-authors>Yangguang Li,</span></li><span class=portfolio__meta-item-authors>Yanpei Cao,</span></li><span class=portfolio__meta-item-authors>Ding Liang</span></li></ul><p class=desc-publication>While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time optimization of 3D textures. Instead, we focus on the fundamental problem of learning in the UV texture space itself. For the first time, we train a large diffusion model capable of directly generating high-resolution texture maps in a feed-forward manner. To facilitate efficient learning in high-resolution UV spaces, we propose a scalable network architecture that interleaves convolutions on UV maps with attention layers on point clouds. Leveraging this architectural design, we train a 700 million parameter diffusion model that can generate UV texture maps guided by text prompts and single-view images. Once trained, our model naturally supports various extended applications, including text-guided texture inpainting, sparse-view texture completion, and text-driven texture synthesis.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2411.14740 target=_blank rel=noopener>PDF</a>
<a class=portfolio__button href=https://github.com/CVMI-Lab/TEXGen target=_blank rel=noopener>CODE</a>
<a class=portfolio__button href=https://cvmi-lab.github.io/TEXGen/ target=_blank rel=noopener>PROJECT</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>3D Vision</p><p class=tag>Large Models</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--right"><h2>Unidream: Unifying diffusion priors for relightable text-to-3d generation</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>On Hold</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Zexiang Liu,</span></li><span class=portfolio__meta-item-authors>Yangguang Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Sida Peng,</span></li><span class=portfolio__meta-item-authors>Yan-Pei Cao,</span></li><span class=portfolio__meta-item-authors>Xiaojuan Qi,</span></li><span class=portfolio__meta-item-authors>Xiaoshui Huang,</span></li><span class=portfolio__meta-item-authors>Ding Liang,</span></li><span class=portfolio__meta-item-authors>Wanli,</span></li><span class=portfolio__meta-item-authors>Ouyang</span></li></ul><p class=desc-publication>&ldquo;Recent advancements in text-to-3D generation technology have significantly advanced the conversion of textual descriptions into imaginative well-geometrical and finely textured 3D objects. Despite these developments, a prevalent limitation arises from the use of RGB data in diffusion or reconstruction models, which often results in models with inherent lighting and shadows effects that detract from their realism, thereby limiting their usability in applications that demand accurate relighting capabilities. To bridge this gap, we present UniDream, a text-to-3D generation framework by incorporating unified diffusion priors. Our approach consists of three main components: (1) a dual-phase training process to get albedo-normal aligned multi-view diffusion and reconstruction models, (2) a progressive generation procedure for geometry and albedo-textures based on Score Distillation Sample (SDS) using the trained reconstruction and diffusion models, and (3) an innovative application of SDS for finalizing PBR generation while keeping a fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate that UniDream surpasses existing methods in generating 3D objects with clearer albedo textures, smoother surfaces, enhanced realism, and superior relighting capabilities.&rdquo;</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2312.08754 target=_blank rel=noopener>PDF</a>
<a class=portfolio__button href=https://github.com/YG256Li/UniDream target=_blank rel=noopener>CODE</a>
<a class=portfolio__button href=https://yg256li.github.io/UniDream/ target=_blank rel=noopener>PROJECT</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>3D Vision</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--left"><h2>Text-to-3D with Classifier Score Distillation</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Yuan-Chen Guo,</span></li><span class=portfolio__meta-item-authors>Yangguang Li,</span></li><span class=portfolio__meta-item-authors>Ding Liang,</span></li><span class=portfolio__meta-item-authors>Song-Hai Zhang,</span></li><span class=portfolio__meta-item-authors>Xiaojuan Qi</span></li></ul><p class=desc-publication>Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding, i.e., the guidance alone is enough for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights for understanding existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2312.08754 target=_blank rel=noopener>PDF</a>
<a class=portfolio__button href=https://github.com/YG256Li/UniDream target=_blank rel=noopener>CODE</a>
<a class=portfolio__button href=https://yg256li.github.io/UniDream/ target=_blank rel=noopener>PROJECT</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>3D Vision</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--right"><h2>Image Inpainting via Iteratively Decoupled Probabilistic Modeling</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Wenbo Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Kun Zhou,</span></li><span class=portfolio__meta-item-authors>Yibing Song,</span></li><span class=portfolio__meta-item-authors>Zhe Lin,</span></li><span class=portfolio__meta-item-authors>Jiaya Jia</span></li></ul><p class=desc-publication>Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2212.02963 target=_blank rel=noopener>PDF</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>Image Synthesis</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--left"><h2>Hybrid neural rendering for large-scale scenes with motion blur</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Wenbo Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Kun Zhou,</span></li><span class=portfolio__meta-item-authors>Yibing Song,</span></li><span class=portfolio__meta-item-authors>Zhe Lin,</span></li><span class=portfolio__meta-item-authors>Jiaya Jia</span></li></ul><p class=desc-publication>Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2212.02963 target=_blank rel=noopener>PDF</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>Image Synthesis</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--right"><h2>Hybrid neural rendering for large-scale scenes with motion blur</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Wenbo Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Kun Zhou,</span></li><span class=portfolio__meta-item-authors>Yibing Song,</span></li><span class=portfolio__meta-item-authors>Zhe Lin,</span></li><span class=portfolio__meta-item-authors>Jiaya Jia</span></li></ul><p class=desc-publication>Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2212.02963 target=_blank rel=noopener>PDF</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>Image Synthesis</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--left"><h2>Hybrid neural rendering for large-scale scenes with motion blur</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Wenbo Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Kun Zhou,</span></li><span class=portfolio__meta-item-authors>Yibing Song,</span></li><span class=portfolio__meta-item-authors>Zhe Lin,</span></li><span class=portfolio__meta-item-authors>Jiaya Jia</span></li></ul><p class=desc-publication>Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2212.02963 target=_blank rel=noopener>PDF</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>Image Synthesis</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--right"><h2>Hybrid neural rendering for large-scale scenes with motion blur</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Wenbo Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Kun Zhou,</span></li><span class=portfolio__meta-item-authors>Yibing Song,</span></li><span class=portfolio__meta-item-authors>Zhe Lin,</span></li><span class=portfolio__meta-item-authors>Jiaya Jia</span></li></ul><p class=desc-publication>Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2212.02963 target=_blank rel=noopener>PDF</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>Image Synthesis</p></div></div></div><div class=portfolio><div class="portfolio__description
portfolio__description--left"><h2>Hybrid neural rendering for large-scale scenes with motion blur</h2><ul class=portfolio__meta><li class=portfolio__meta-item><em class="fas fa-flag-checkered"></em>
<span>Finished</span></li><em class="fas fa-pencil"></em>&nbsp
               
            
<span class=portfolio__meta-item-authors>Wenbo Li,</span></li><span class=portfolio__meta-item-authors-me>Xin Yu,</span></li><span class=portfolio__meta-item-authors>Kun Zhou,</span></li><span class=portfolio__meta-item-authors>Yibing Song,</span></li><span class=portfolio__meta-item-authors>Zhe Lin,</span></li><span class=portfolio__meta-item-authors>Jiaya Jia</span></li></ul><p class=desc-publication>Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.</p><div class=portfolio__button-wrapper><a class=portfolio__button href=https://arxiv.org/abs/2212.02963 target=_blank rel=noopener>PDF</a></div><div class=separator><p class=tag>Generative Model</p><p class=tag>Image Synthesis</p></div></div></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Xin Yu
2024</li></ul></footer><script type=text/javascript src=/hugo-anatole/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>